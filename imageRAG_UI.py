import streamlit as st
import torch
import os
import base64
import gc
from io import BytesIO
from PIL import Image
import numpy as np
import openai
from diffusers import AutoPipelineForText2Image, DiffusionPipeline
from transformers import CLIPVisionModelWithProjection

# ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from imageRAG_SDXL import * 
from utils import *
from retrieval import *

# --- è¨­å®šã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
def get_image_download_link(img, filename="generated.png", text="ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
    buffered = BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:image/png;base64,{img_str}" download="{filename}">{text}</a>'
    return href

def clear_vram():
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
        torch.mps.synchronize()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()

# --- UI æ§‹æˆ ---
st.set_page_config(page_title="ImageRAG ã‚¦ã‚§ãƒ–UI", layout="wide")
st.title("ğŸ–¼ï¸ ImageRAG WebUI")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– ---
if "generating" not in st.session_state:
    st.session_state.generating = False

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
st.sidebar.header("åŸºæœ¬è¨­å®š")
openai_api_key = st.sidebar.text_input("OpenAI API ã‚­ãƒ¼", type="password")

st.sidebar.markdown("---")
st.sidebar.subheader("å‚ç…§ç”»åƒã®é¸æŠ")

source_choice = st.sidebar.radio(
    "ã©ã¡ã‚‰ã®æ–¹æ³•ã§ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã‹ï¼Ÿ",
    ("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è‡ªå‹•æ¤œç´¢", "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã‚’å‚ç…§"),
    disabled=st.session_state.generating
)

dataset_name = None
user_uploaded_file = None

if source_choice == "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è‡ªå‹•æ¤œç´¢":
    dataset_root = "datasets"
    if os.path.exists(dataset_root):
        available_datasets = [f for f in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, f))]
    else:
        available_datasets = []
    
    if available_datasets:
        dataset_name = st.sidebar.selectbox("ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠ", available_datasets, disabled=st.session_state.generating)
        dataset_path = f"datasets/{dataset_name}"
        
        with st.sidebar.expander("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»åƒã‚’ç¢ºèªã™ã‚‹", expanded=False):
            preview_images = [f for f in os.listdir(dataset_path) if f.endswith(('.png', '.jpg', '.jpeg'))]
            if preview_images:
                st.write(f"åˆè¨ˆ: {len(preview_images)} æš")
                for img_file in preview_images[:10]: 
                    img_ptr = Image.open(os.path.join(dataset_path, img_file))
                    st.image(img_ptr, caption=img_file, use_container_width=True)
else:
    st.sidebar.info("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã‚’å…ƒã«ä¸è¶³è¦ç´ ã‚’è£œå®Œã—ã¾ã™ã€‚")
    user_uploaded_file = st.sidebar.file_uploader("å‚ç…§ã—ãŸã„ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["png", "jpg", "jpeg"], disabled=st.session_state.generating)

st.sidebar.markdown("---")
out_name = st.sidebar.text_input("å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å", value="generated_image", disabled=st.session_state.generating)
ip_scale = st.sidebar.slider("IP-Adapter å¼·åº¦", 0.0, 1.0, 0.4, disabled=st.session_state.generating)

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
st.info("ğŸ’¡ **Tips:** æ—¥æœ¬èªã§ã‚‚å…¥åŠ›å¯èƒ½ã§ã™ãŒã€è‹±èªã®æ–¹ãŒã‚ˆã‚Šæ­£ç¢ºãªç”»åƒãŒç”Ÿæˆã•ã‚Œã‚„ã™ããªã‚Šã¾ã™ã€‚")
prompt = st.text_area(
    "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›", 
    placeholder="ä¾‹ï¼šA golden retriever and a cradle. (æ—¥æœ¬èªã§ã®å…¥åŠ›ã‚‚è‡ªå‹•ç¿»è¨³ã•ã‚Œã¾ã™)", 
    disabled=st.session_state.generating
)

# ãƒœã‚¿ãƒ³åˆ¶å¾¡
def start_generation():
    if not openai_api_key:
        st.error("OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.session_state.generating = True

def stop_generation():
    st.session_state.generating = False

col_run, col_stop = st.columns([1, 4])
with col_run:
    st.button("ç”»åƒç”Ÿæˆã‚’é–‹å§‹", on_click=start_generation, disabled=st.session_state.generating)

with col_stop:
    if st.session_state.generating:
        st.button("ç”»åƒç”Ÿæˆã‚’ä¸­æ­¢", on_click=stop_generation)

# --- ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ ---
if st.session_state.generating:
    client = openai.OpenAI(api_key=openai_api_key)
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    
    negative_prompt = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, monochrome"

    os.makedirs("results", exist_ok=True)
    col1, col2 = st.columns(2)
    
    try:
        def latents_callback(pipe, step, timestep, callback_kwargs):
            if not st.session_state.generating:
                raise RuntimeError("Manual Stop")
            return callback_kwargs

        with st.status("ImageRAG å®Ÿè¡Œä¸­...", expanded=True) as status:
            # Step 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç¿»è¨³ã¨åˆæœŸç”Ÿæˆ
            status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è§£æä¸­...")
            rephrased_prompt = get_enhanced_rephrased_prompt(prompt, client)
            st.write(f"**ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:** {rephrased_prompt}")

            status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 1: åˆæœŸã®ç”»åƒã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...")
            pipe = AutoPipelineForText2Image.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0", 
                torch_dtype=torch.float16, variant="fp16", use_safetensors=True
            ).to(device)
            pipe.enable_vae_slicing()
            pipe.enable_vae_tiling()
            
            generator = torch.Generator(device="cpu").manual_seed(42)
            
            init_image = pipe(
                prompt=rephrased_prompt, 
                negative_prompt=negative_prompt,
                num_inference_steps=35, 
                generator=generator,
                callback_on_step_end=latents_callback
            ).images[0]
            
            col1.image(init_image, caption="åˆæœŸç”Ÿæˆç”»åƒ")
            del pipe
            clear_vram()
            
            if not st.session_state.generating: raise RuntimeError("Manual Stop")

            # Step 2: åˆ¤å®š
            status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 2: AIãŒå†…å®¹ã‚’ç¢ºèªä¸­...")
            temp_path = "results/temp_init.png"
            init_image.save(temp_path)
            decision, _ = decision_making(prompt, [temp_path], client)
            
            if not st.session_state.generating: raise RuntimeError("Manual Stop")

            if "YES" in decision.upper():
                st.success("åˆæœŸç”»åƒã§å®Œæˆã§ã™ï¼")
                final_image = init_image
            else:
                # Step 3: å‚ç…§ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®æº–å‚™
                status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 3: ä¸è¶³è¦ç´ ã‚’è£œå®Œã™ã‚‹ãŸã‚ã®æƒ…å ±ã‚’åé›†ä¸­...")
                
                # caption ã®å®šç¾©
                caption = "selected reference detail" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

                ref_image_final = None
                
                if source_choice == "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã‚’å‚ç…§" and user_uploaded_file:
                    status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 3: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’ä½¿ç”¨ã—ã¾ã™...")
                    ref_image_final = Image.open(user_uploaded_file).convert("RGB")
                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸»è¦ãªæ¦‚å¿µã‚’ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨ã™ã‚‹
                    caption_res = retrieval_caption_generation(prompt, [temp_path], client, k_captions_per_concept=1)
                    caption = convert_res_to_captions(caption_res)[0]
                
                elif source_choice == "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰è‡ªå‹•æ¤œç´¢" and dataset_name:
                    status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å‚ç…§ç”»åƒã‚’æ¤œç´¢ä¸­...")
                    dataset_path = f"datasets/{dataset_name}"
                    retrieval_image_paths = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(('.png', '.jpg', '.jpeg'))]
                    embeddings_path = f"{dataset_path}/embeddings"
                    
                    caption_res = retrieval_caption_generation(prompt, [temp_path], client, k_captions_per_concept=1)
                    caption = convert_res_to_captions(caption_res)[0]
                    paths = retrieve_img_per_caption([caption], retrieval_image_paths, embeddings_path=embeddings_path, k=1, device=device)
                    ref_image_path = np.array(paths).flatten()[0]
                    ref_image_final = Image.open(ref_image_path)
                    st.image(ref_image_final, caption=f"æ¤œç´¢ã•ã‚ŒãŸç”»åƒ: {caption}", width=300)

                if not st.session_state.generating: raise RuntimeError("Manual Stop")

                if ref_image_final is None:
                    st.warning("å‚ç…§ç”»åƒãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€åˆæœŸç”»åƒã‚’æœ€çµ‚çµæœã¨ã—ã¾ã™ã€‚")
                    final_image = init_image
                else:
                    # Step 4: å†ç”Ÿæˆ
                    status.update(label="ã‚¹ãƒ†ãƒƒãƒ— 4: å‚ç…§ç”»åƒã‚’é©ç”¨ã—ã¦å†ç”Ÿæˆä¸­...")
                    image_encoder = CLIPVisionModelWithProjection.from_pretrained(
                        "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", torch_dtype=torch.float16
                    ).to(device)
                    
                    pipe_ip = DiffusionPipeline.from_pretrained(
                        "stabilityai/stable-diffusion-xl-base-1.0", image_encoder=image_encoder,
                        torch_dtype=torch.float16, variant="fp16", use_safetensors=True
                    ).to(device)
                    
                    pipe_ip.load_ip_adapter("h94/IP-Adapter", subfolder="sdxl_models", weight_name="ip-adapter_sdxl.bin")
                    pipe_ip.enable_vae_slicing()
                    pipe_ip.enable_vae_tiling()
                    pipe_ip.set_ip_adapter_scale(ip_scale)
                    
                    # å†ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ„ã¿ç«‹ã¦
                    new_prompt = f"According to this image of {caption}, improve the following scene: {rephrased_prompt}"
                    
                    final_image = pipe_ip(
                        prompt=new_prompt,
                        ip_adapter_image=ref_image_final,
                        negative_prompt=negative_prompt,
                        num_inference_steps=35,
                        generator=generator,
                        callback_on_step_end=latents_callback
                    ).images[0]
                    
                    del pipe_ip, image_encoder
                    clear_vram()

            col2.image(final_image, caption="æœ€çµ‚ç”»åƒå‡ºåŠ›")
            status.update(label="å®Œäº†ï¼", state="complete")
            st.session_state.generating = False

        st.markdown("---")
        st.markdown(get_image_download_link(final_image, f"{out_name}.png", "ç”Ÿæˆã•ã‚ŒãŸç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"), unsafe_allow_html=True)

    except RuntimeError as e:
        if str(e) == "Manual Stop":
            st.warning("ç”ŸæˆãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ­¢ã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.error(f"å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.session_state.generating = False
        clear_vram()
        st.rerun()
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.session_state.generating = False
        clear_vram()